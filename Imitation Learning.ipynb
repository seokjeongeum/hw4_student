{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imitation Learning with Neural Network Policies\n",
    "In this notebook, you will implement the supervised losses for behavior cloning and use it to train policies for locomotion tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title imports\n",
    "# As usual, a bit of setup\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import deeprl.infrastructure.pytorch_util as ptu\n",
    "\n",
    "from deeprl.infrastructure.rl_trainer import RL_Trainer\n",
    "from deeprl.infrastructure.trainers import BC_Trainer\n",
    "from deeprl.agents.bc_agent import BCAgent\n",
    "from deeprl.policies.loaded_gaussian_policy import LoadedGaussianPolicy\n",
    "from deeprl.policies.MLP_policy import MLPPolicySL\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "def remove_folder(path):\n",
    "    # check if folder exists\n",
    "    if os.path.exists(path): \n",
    "        print(\"Clearing old results at {}\".format(path))\n",
    "        # remove if exists\n",
    "        shutil.rmtree(path)\n",
    "    else:\n",
    "        print(\"Folder {} does not exist yet. No old results to delete\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_base_args_dict = dict(\n",
    "    expert_policy_file = 'deeprl/policies/experts/Hopper.pkl', #@param\n",
    "    expert_data = 'deeprl/expert_data/expert_data_Hopper-v2.pkl', #@param\n",
    "    env_name = 'Hopper-v2', #@param ['Ant-v2', 'Humanoid-v2', 'Walker2d-v2', 'HalfCheetah-v2', 'Hopper-v2']\n",
    "    exp_name = 'test_bc', #@param\n",
    "    do_dagger = True, #@param {type: \"boolean\"}\n",
    "    ep_len = 1000, #@param {type: \"integer\"}\n",
    "    save_params = False, #@param {type: \"boolean\"}\n",
    "\n",
    "    # Training\n",
    "    num_agent_train_steps_per_iter = 1000, #@param {type: \"integer\"})\n",
    "    n_iter = 1, #@param {type: \"integer\"})\n",
    "\n",
    "    # batches & buffers\n",
    "    batch_size = 10000, #@param {type: \"integer\"})\n",
    "    eval_batch_size = 1000, #@param {type: \"integer\"}\n",
    "    train_batch_size = 100, #@param {type: \"integer\"}\n",
    "    max_replay_buffer_size = 1000000, #@param {type: \"integer\"}\n",
    "\n",
    "    #@markdown network\n",
    "    n_layers = 2, #@param {type: \"integer\"}\n",
    "    size = 64, #@param {type: \"integer\"}\n",
    "    learning_rate = 5e-3, #@param {type: \"number\"}\n",
    "\n",
    "    #@markdown logging\n",
    "    video_log_freq = -1, #@param {type: \"integer\"}\n",
    "    scalar_log_freq = 1, #@param {type: \"integer\"}\n",
    "\n",
    "    #@markdown gpu & run-time settings\n",
    "    no_gpu = False, #@param {type: \"boolean\"}\n",
    "    which_gpu = 0, #@param {type: \"integer\"}\n",
    "    seed = 2, #@param {type: \"integer\"}\n",
    "    logdir = 'test',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infrastructure\n",
    "**Policies**: We have provided implementations of simple neural network policies for your convenience. For discrete environments, the neural network takes in the current state and outputs the logits of the policy's action distribution at this state. The policy then outputs a categorical distribution using those logits. In environments with continuous action spaces, the network will output the mean of a diagonal Gaussian distribution, as well as having a separate single parameter for the log standard deviations of the Gaussian. \n",
    "\n",
    "Calling forward on the policy will output a torch distribution object, so look at the documentation at https://pytorch.org/docs/stable/distributions.html.\n",
    "Look at <code>policies/MLP_policy</code> to make sure you understand the implementation.\n",
    "\n",
    "**RL Training Loop**: The reinforcement learning training loop, which alternates between gathering samples from the environment and updating the policy (and other learned functions) can be found in <code>infrastructure/rl_trainer.py</code>. While you won't need to understand this for the basic behavior cloning part (as you only use a fixed set of expert data), you should read through and understand the run_training_loop function before starting the Dagger implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Behavior Cloning\n",
    "The first part of the assignment will be a familiar exercise in supervised learning. Given a dataset of expert trajectories, we will simply train our policy to imitate the expert via maximum likelihood. Fill out the update method in the MLPPolicySL class in <code>policies/MLP_policy.py</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight before update [[-0.00432252  0.30971584 -0.47518533]\n",
      " [-0.4248946  -0.22236897  0.15482073]]\n",
      "2.628419\n",
      "Loss Error 1.4757100142599302e-08 should be on the order of 1e-6 or lower\n",
      "Weight after update [[ 0.03953294 -0.15170135 -1.5365069 ]\n",
      " [-0.21503024 -1.4284426  -0.84785604]]\n",
      "Change in weights [[ 0.04385546 -0.4614172  -1.0613215 ]\n",
      " [ 0.20986436 -1.2060736  -1.0026767 ]]\n",
      "Weight Update Error 2.5347942329768053e-08 should be on the order of 1e-6 or lower\n"
     ]
    }
   ],
   "source": [
    "### Basic test for correctness of loss and gradients\n",
    "torch.manual_seed(0)\n",
    "ac_dim = 2\n",
    "ob_dim = 3\n",
    "batch_size = 5\n",
    "\n",
    "policy = MLPPolicySL(\n",
    "            ac_dim=ac_dim,\n",
    "            ob_dim=ob_dim,\n",
    "            n_layers=1,\n",
    "            size=2,\n",
    "            learning_rate=0.25)\n",
    "\n",
    "np.random.seed(0)\n",
    "obs = np.random.normal(size=(batch_size, ob_dim))\n",
    "acts = np.random.normal(size=(batch_size, ac_dim))\n",
    "\n",
    "first_weight_before = np.array(ptu.to_numpy(next(policy.mean_net.parameters())))\n",
    "print(\"Weight before update\", first_weight_before)\n",
    "\n",
    "for i in range(5):\n",
    "    loss = policy.update(obs, acts)['Training Loss']\n",
    "\n",
    "print(loss)\n",
    "expected_loss = 2.628419\n",
    "loss_error = rel_error(loss, expected_loss)\n",
    "print(\"Loss Error\", loss_error, \"should be on the order of 1e-6 or lower\")\n",
    "\n",
    "first_weight_after = ptu.to_numpy(next(policy.mean_net.parameters()))\n",
    "print('Weight after update', first_weight_after)\n",
    "\n",
    "weight_change = first_weight_after - first_weight_before\n",
    "print(\"Change in weights\", weight_change)\n",
    "\n",
    "expected_change = np.array([[ 0.04385546, -0.4614172,  -1.0613215 ],\n",
    "                            [ 0.20986436, -1.2060736,  -1.0026767 ]])\n",
    "updated_weight_error = rel_error(weight_change, expected_change)\n",
    "print(\"Weight Update Error\", updated_weight_error, \"should be on the order of 1e-6 or lower\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having implemented our behavior cloning loss, we can now start training some policies to imitate the expert policies provided. \n",
    "\n",
    "Run the following cell to train policies with simple behavior cloning on the HalfCheetah environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LD_LIBRARY_PATH: /usr/lib/x86_64-linux-gnu:/root/.mujoco/mujoco200/bin/\n",
      "Clearing old results at logs/behavior_cloning/HalfCheetah\n",
      "Running behavior cloning experiment with seed 0\n",
      "########################\n",
      "logging outputs to  logs/behavior_cloning/HalfCheetah/seed0\n",
      "########################\n",
      "Using CPU for this assignment. There may be some bugs with using GPU that cause test cases to not match. You can uncomment the code below if you want to try using it.\n",
      "HalfCheetah-v2\n",
      "Loading expert policy from... deeprl/policies/experts/HalfCheetah.pkl\n",
      "obs (1, 17) (1, 17)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 3714.16162109375\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 3714.16162109375\n",
      "Eval_MinReturn : 3714.16162109375\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4205.7783203125\n",
      "Train_StdReturn : 83.038818359375\n",
      "Train_MaxReturn : 4288.81689453125\n",
      "Train_MinReturn : 4122.7392578125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 1.7457103729248047\n",
      "Training Loss : -7.843830585479736\n",
      "Initial_DataCollection_AverageReturn : 4205.7783203125\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Running behavior cloning experiment with seed 1\n",
      "########################\n",
      "logging outputs to  logs/behavior_cloning/HalfCheetah/seed1\n",
      "########################\n",
      "Using CPU for this assignment. There may be some bugs with using GPU that cause test cases to not match. You can uncomment the code below if you want to try using it.\n",
      "HalfCheetah-v2\n",
      "Loading expert policy from... deeprl/policies/experts/HalfCheetah.pkl\n",
      "obs (1, 17) (1, 17)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 3975.2138671875\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 3975.2138671875\n",
      "Eval_MinReturn : 3975.2138671875\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4205.7783203125\n",
      "Train_StdReturn : 83.038818359375\n",
      "Train_MaxReturn : 4288.81689453125\n",
      "Train_MinReturn : 4122.7392578125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 1.5800089836120605\n",
      "Training Loss : -8.353324890136719\n",
      "Initial_DataCollection_AverageReturn : 4205.7783203125\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Running behavior cloning experiment with seed 2\n",
      "########################\n",
      "logging outputs to  logs/behavior_cloning/HalfCheetah/seed2\n",
      "########################\n",
      "Using CPU for this assignment. There may be some bugs with using GPU that cause test cases to not match. You can uncomment the code below if you want to try using it.\n",
      "HalfCheetah-v2\n",
      "Loading expert policy from... deeprl/policies/experts/HalfCheetah.pkl\n",
      "obs (1, 17) (1, 17)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 4003.567626953125\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 4003.567626953125\n",
      "Eval_MinReturn : 4003.567626953125\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4205.7783203125\n",
      "Train_StdReturn : 83.038818359375\n",
      "Train_MaxReturn : 4288.81689453125\n",
      "Train_MinReturn : 4122.7392578125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 1.652512550354004\n",
      "Training Loss : -9.084877014160156\n",
      "Initial_DataCollection_AverageReturn : 4205.7783203125\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Prepend the system library path to the current LD_LIBRARY_PATH (if it exists)\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = \"/usr/lib/x86_64-linux-gnu:\" + os.environ.get(\"LD_LIBRARY_PATH\", \"\")\n",
    "print(\"LD_LIBRARY_PATH:\", os.environ[\"LD_LIBRARY_PATH\"])\n",
    "bc_args = dict(bc_base_args_dict)\n",
    "\n",
    "env_str = 'HalfCheetah'\n",
    "bc_args['expert_policy_file'] = 'deeprl/policies/experts/{}.pkl'.format(env_str)\n",
    "bc_args['expert_data'] = 'deeprl/expert_data/expert_data_{}-v2.pkl'.format(env_str)\n",
    "bc_args['env_name'] = '{}-v2'.format(env_str)\n",
    "\n",
    "# Delete all previous logs\n",
    "remove_folder('logs/behavior_cloning/{}'.format(env_str))\n",
    "\n",
    "for seed in range(3):\n",
    "    print(\"Running behavior cloning experiment with seed\", seed)\n",
    "    bc_args['seed'] = seed\n",
    "    bc_args['logdir'] = 'logs/behavior_cloning/{}/seed{}'.format(env_str, seed)\n",
    "    bctrainer = BC_Trainer(bc_args)\n",
    "    bctrainer.run_training_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize your results using Tensorboard. You should see that on HalfCheetah, the returns of your learned policies (Eval_AverageReturn) are fairly similar (thought a bit lower) to that of the expert (Initial_DataCollection_Average_Return)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 68281), started 0:08:30 ago. (Use '!kill 68281' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1dba8eba917e4600\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1dba8eba917e4600\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Visualize behavior cloning results on HalfCheetah\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/behavior_cloning/HalfCheetah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the following cell to train policies with simple behavior cloning on Hopper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder logs/behavior_cloning/Hopper does not exist yet. No old results to delete\n",
      "Running behavior cloning experiment on Hopper with seed 0\n",
      "########################\n",
      "logging outputs to  logs/behavior_cloning/Hopper/seed0\n",
      "########################\n",
      "Using CPU for this assignment. There may be some bugs with using GPU that cause test cases to not match. You can uncomment the code below if you want to try using it.\n",
      "Hopper-v2\n",
      "Loading expert policy from... deeprl/policies/experts/Hopper.pkl\n",
      "obs (1, 11) (1, 11)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 776.6627197265625\n",
      "Eval_StdReturn : 287.67132568359375\n",
      "Eval_MaxReturn : 1169.0322265625\n",
      "Eval_MinReturn : 441.0273742675781\n",
      "Eval_AverageEpLen : 242.8\n",
      "Train_AverageReturn : 3772.67041015625\n",
      "Train_StdReturn : 1.9483642578125\n",
      "Train_MaxReturn : 3774.61865234375\n",
      "Train_MinReturn : 3770.721923828125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 1.9736623764038086\n",
      "Training Loss : -3.9659242630004883\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Running behavior cloning experiment on Hopper with seed 1\n",
      "########################\n",
      "logging outputs to  logs/behavior_cloning/Hopper/seed1\n",
      "########################\n",
      "Using CPU for this assignment. There may be some bugs with using GPU that cause test cases to not match. You can uncomment the code below if you want to try using it.\n",
      "Hopper-v2\n",
      "Loading expert policy from... deeprl/policies/experts/Hopper.pkl\n",
      "obs (1, 11) (1, 11)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 1029.3245849609375\n",
      "Eval_StdReturn : 106.21780395507812\n",
      "Eval_MaxReturn : 1165.327880859375\n",
      "Eval_MinReturn : 906.8502807617188\n",
      "Eval_AverageEpLen : 299.25\n",
      "Train_AverageReturn : 3772.67041015625\n",
      "Train_StdReturn : 1.9483642578125\n",
      "Train_MaxReturn : 3774.61865234375\n",
      "Train_MinReturn : 3770.721923828125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 1.8080732822418213\n",
      "Training Loss : -3.559098243713379\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Running behavior cloning experiment on Hopper with seed 2\n",
      "########################\n",
      "logging outputs to  logs/behavior_cloning/Hopper/seed2\n",
      "########################\n",
      "Using CPU for this assignment. There may be some bugs with using GPU that cause test cases to not match. You can uncomment the code below if you want to try using it.\n",
      "Hopper-v2\n",
      "Loading expert policy from... deeprl/policies/experts/Hopper.pkl\n",
      "obs (1, 11) (1, 11)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 924.69482421875\n",
      "Eval_StdReturn : 197.03053283691406\n",
      "Eval_MaxReturn : 1235.950439453125\n",
      "Eval_MinReturn : 754.0817260742188\n",
      "Eval_AverageEpLen : 293.0\n",
      "Train_AverageReturn : 3772.67041015625\n",
      "Train_StdReturn : 1.9483642578125\n",
      "Train_MaxReturn : 3774.61865234375\n",
      "Train_MinReturn : 3770.721923828125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 1.8402962684631348\n",
      "Training Loss : -3.5931427478790283\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bc_args = dict(bc_base_args_dict)\n",
    "\n",
    "env_str = 'Hopper'\n",
    "bc_args['expert_policy_file'] = 'deeprl/policies/experts/{}.pkl'.format(env_str)\n",
    "bc_args['expert_data'] = 'deeprl/expert_data/expert_data_{}-v2.pkl'.format(env_str)\n",
    "bc_args['env_name'] = '{}-v2'.format(env_str)\n",
    "\n",
    "# Delete all previous logs\n",
    "remove_folder('logs/behavior_cloning/{}'.format(env_str))\n",
    "\n",
    "for seed in range(3):\n",
    "    print(\"Running behavior cloning experiment on Hopper with seed\", seed)\n",
    "    bc_args['seed'] = seed\n",
    "    bc_args['logdir'] = 'logs/behavior_cloning/{}/seed{}'.format(env_str, seed)\n",
    "    bctrainer = BC_Trainer(bc_args)\n",
    "    bctrainer.run_training_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize your results using Tensorboard. You should see that on Hopper, the returns of your learned policies (Eval_AverageReturn) are substantially lower than that of the expert (Initial_DataCollection_Average_Return), due to the distribution shift issues that arise when doing naive behavior cloning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 68391), started 0:06:21 ago. (Use '!kill 68391' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-a3ec8aabefa9e26c\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-a3ec8aabefa9e26c\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Visualize behavior cloning results on Hopper\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/behavior_cloning/Hopper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Aggregation\n",
    "As discussed in lecture, behavior cloning can suffer from distribution shift, as a small mismatch between the learned and expert policy can take the learned policy to new states that were unseen during training, on which the learned policy hasn't been trained. In Dagger, we will address this issue iteratively, where we use our expert policy to provide labels for the new states we encounter with our learned policy, and then retrain our policy on these newly labeled states.\n",
    "\n",
    "Implement the <code>do_relabel_with_expert</code> function in <code>infrastructure/rl_trainer.py</code>. The errors in the expert actions should be on the order of 1e-6 or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################\n",
      "logging outputs to  test\n",
      "########################\n",
      "Using CPU for this assignment. There may be some bugs with using GPU that cause test cases to not match. You can uncomment the code below if you want to try using it.\n",
      "Hopper-v2\n",
      "Loading expert policy from... deeprl/policies/experts/Hopper.pkl\n",
      "obs (1, 11) (1, 11)\n",
      "Done restoring expert policy...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "Path 0 expert action error 1.0\n",
      "Path 1 expert action error 1.0\n",
      "Path 2 expert action error 1.0\n"
     ]
    }
   ],
   "source": [
    "### Test do relabel function\n",
    "bc_args = dict(bc_base_args_dict)\n",
    "\n",
    "env_str = 'Hopper'\n",
    "bc_args['expert_policy_file'] = 'deeprl/policies/experts/{}.pkl'.format(env_str)\n",
    "bc_args['expert_data'] = 'deeprl/expert_data/expert_data_{}-v2.pkl'.format(env_str)\n",
    "bc_args['env_name'] = '{}-v2'.format(env_str)\n",
    "bctrainer = BC_Trainer(bc_args)\n",
    "\n",
    "np.random.seed(0)\n",
    "T = 2\n",
    "ob_dim = 11\n",
    "ac_dim = 3\n",
    "\n",
    "paths = []\n",
    "for i in range(3):\n",
    "    obs = np.random.normal(size=(T, ob_dim))\n",
    "    acs = np.random.normal(size=(T, ac_dim))\n",
    "    paths.append(dict(observation=obs,\n",
    "                      action=acs))\n",
    "    \n",
    "rl_trainer = bctrainer.rl_trainer\n",
    "relabeled_paths = rl_trainer.do_relabel_with_expert(bctrainer.loaded_expert_policy, paths)\n",
    "\n",
    "expert_actions = np.array([[[-1.7814021, -0.11137983,  1.763353  ],\n",
    "                            [-2.589222,   -5.463195,    2.4301376 ]],\n",
    "                           [[-2.8287444, -5.298558,   3.0320463],\n",
    "                            [ 3.9611065,  2.626403,  -2.8639293]],\n",
    "                           [[-0.3055225,  -0.9865407,   0.80830705],\n",
    "                            [ 2.8788857,   3.5550566,  -0.92875874]]])\n",
    "\n",
    "for i, (path, relabeled_path) in enumerate(zip(paths, relabeled_paths)):\n",
    "    assert np.all(path['observation'] == relabeled_path['observation'])\n",
    "    print(\"Path {} expert action error\".format(i), rel_error(expert_actions[i], relabeled_path['action']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run Dagger on the Hopper env again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dagger_args = dict(bc_base_args_dict)\n",
    "\n",
    "dagger_args['do_dagger'] = True\n",
    "dagger_args['n_iter'] = 10\n",
    "\n",
    "env_str = 'Hopper'\n",
    "dagger_args['expert_policy_file'] = 'deeprl/policies/experts/{}.pkl'.format(env_str)\n",
    "dagger_args['expert_data'] = 'deeprl/expert_data/expert_data_{}-v2.pkl'.format(env_str)\n",
    "dagger_args['env_name'] = '{}-v2'.format(env_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder logs/dagger/Hopper does not exist yet. No old results to delete\n",
      "Running Dagger experiment with seed 0\n",
      "########################\n",
      "logging outputs to  logs/dagger/Hopper/seed0\n",
      "########################\n",
      "Using CPU for this assignment. There may be some bugs with using GPU that cause test cases to not match. You can uncomment the code below if you want to try using it.\n",
      "Hopper-v2\n",
      "Loading expert policy from... deeprl/policies/experts/Hopper.pkl\n",
      "obs (1, 11) (1, 11)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 776.6627197265625\n",
      "Eval_StdReturn : 287.67132568359375\n",
      "Eval_MaxReturn : 1169.0322265625\n",
      "Eval_MinReturn : 441.0273742675781\n",
      "Eval_AverageEpLen : 242.8\n",
      "Train_AverageReturn : 3772.67041015625\n",
      "Train_StdReturn : 1.9483642578125\n",
      "Train_MaxReturn : 3774.61865234375\n",
      "Train_MinReturn : 3770.721923828125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 2.1371970176696777\n",
      "Training Loss : -3.9659242630004883\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     10184 / 10000\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 821.228515625\n",
      "Eval_StdReturn : 387.6514587402344\n",
      "Eval_MaxReturn : 1327.218017578125\n",
      "Eval_MinReturn : 332.9251708984375\n",
      "Eval_AverageEpLen : 254.6\n",
      "Train_AverageReturn : 809.4156494140625\n",
      "Train_StdReturn : 403.3546447753906\n",
      "Train_MaxReturn : 1825.246337890625\n",
      "Train_MinReturn : 402.11065673828125\n",
      "Train_AverageEpLen : 261.12820512820514\n",
      "Train_EnvstepsSoFar : 10184\n",
      "TimeSinceStart : 8.853822946548462\n",
      "Training Loss : -3.007702112197876\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     10086 / 10000\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 684.5724487304688\n",
      "Eval_StdReturn : 213.1841583251953\n",
      "Eval_MaxReturn : 884.6251220703125\n",
      "Eval_MinReturn : 399.0592041015625\n",
      "Eval_AverageEpLen : 216.8\n",
      "Train_AverageReturn : 725.0387573242188\n",
      "Train_StdReturn : 350.60546875\n",
      "Train_MaxReturn : 1688.763427734375\n",
      "Train_MinReturn : 359.5780029296875\n",
      "Train_AverageEpLen : 240.14285714285714\n",
      "Train_EnvstepsSoFar : 20270\n",
      "TimeSinceStart : 15.237466812133789\n",
      "Training Loss : -2.8017754554748535\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     10327 / 10000\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 1104.623046875\n",
      "Eval_StdReturn : 273.90850830078125\n",
      "Eval_MaxReturn : 1458.246337890625\n",
      "Eval_MinReturn : 790.8743896484375\n",
      "Eval_AverageEpLen : 334.6666666666667\n",
      "Train_AverageReturn : 861.3984375\n",
      "Train_StdReturn : 359.98126220703125\n",
      "Train_MaxReturn : 1834.25341796875\n",
      "Train_MinReturn : 341.90765380859375\n",
      "Train_AverageEpLen : 264.79487179487177\n",
      "Train_EnvstepsSoFar : 30597\n",
      "TimeSinceStart : 22.16759753227234\n",
      "Training Loss : -2.6032145023345947\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     10319 / 10000\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 690.2093505859375\n",
      "Eval_StdReturn : 347.7223205566406\n",
      "Eval_MaxReturn : 1193.439697265625\n",
      "Eval_MinReturn : 334.15057373046875\n",
      "Eval_AverageEpLen : 229.0\n",
      "Train_AverageReturn : 753.0745239257812\n",
      "Train_StdReturn : 397.3076477050781\n",
      "Train_MaxReturn : 1716.6715087890625\n",
      "Train_MinReturn : 289.978271484375\n",
      "Train_AverageEpLen : 245.6904761904762\n",
      "Train_EnvstepsSoFar : 40916\n",
      "TimeSinceStart : 29.375930070877075\n",
      "Training Loss : -2.751898765563965\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     10229 / 10000\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 754.3211669921875\n",
      "Eval_StdReturn : 327.7669982910156\n",
      "Eval_MaxReturn : 1093.234619140625\n",
      "Eval_MinReturn : 353.4311828613281\n",
      "Eval_AverageEpLen : 232.4\n",
      "Train_AverageReturn : 678.456787109375\n",
      "Train_StdReturn : 384.3249206542969\n",
      "Train_MaxReturn : 1820.009033203125\n",
      "Train_MinReturn : 289.9125671386719\n",
      "Train_AverageEpLen : 227.3111111111111\n",
      "Train_EnvstepsSoFar : 51145\n",
      "TimeSinceStart : 36.78588604927063\n",
      "Training Loss : -2.537587881088257\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     10159 / 10000\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 528.92333984375\n",
      "Eval_StdReturn : 301.6190185546875\n",
      "Eval_MaxReturn : 1197.01708984375\n",
      "Eval_MinReturn : 355.52301025390625\n",
      "Eval_AverageEpLen : 178.33333333333334\n",
      "Train_AverageReturn : 582.0947265625\n",
      "Train_StdReturn : 271.6995544433594\n",
      "Train_MaxReturn : 1540.6207275390625\n",
      "Train_MinReturn : 226.85858154296875\n",
      "Train_AverageEpLen : 199.19607843137254\n",
      "Train_EnvstepsSoFar : 61304\n",
      "TimeSinceStart : 44.33126711845398\n",
      "Training Loss : -2.0895681381225586\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     10125 / 10000\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 465.4473876953125\n",
      "Eval_StdReturn : 249.24160766601562\n",
      "Eval_MaxReturn : 1100.6058349609375\n",
      "Eval_MinReturn : 240.1847381591797\n",
      "Eval_AverageEpLen : 163.375\n",
      "Train_AverageReturn : 565.2216796875\n",
      "Train_StdReturn : 331.54071044921875\n",
      "Train_MaxReturn : 1676.9205322265625\n",
      "Train_MinReturn : 225.87559509277344\n",
      "Train_AverageEpLen : 194.71153846153845\n",
      "Train_EnvstepsSoFar : 71429\n",
      "TimeSinceStart : 52.49613618850708\n",
      "Training Loss : -2.2947194576263428\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     10040 / 10000\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 386.406494140625\n",
      "Eval_StdReturn : 72.17864227294922\n",
      "Eval_MaxReturn : 489.4214782714844\n",
      "Eval_MinReturn : 224.5935516357422\n",
      "Eval_AverageEpLen : 142.25\n",
      "Train_AverageReturn : 384.6579284667969\n",
      "Train_StdReturn : 177.0010223388672\n",
      "Train_MaxReturn : 1077.6982421875\n",
      "Train_MinReturn : 217.4437255859375\n",
      "Train_AverageEpLen : 143.42857142857142\n",
      "Train_EnvstepsSoFar : 81469\n",
      "TimeSinceStart : 60.54636907577515\n",
      "Training Loss : -2.368645191192627\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     10101 / 10000\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 658.5307006835938\n",
      "Eval_StdReturn : 301.83905029296875\n",
      "Eval_MaxReturn : 1119.181396484375\n",
      "Eval_MinReturn : 425.8943176269531\n",
      "Eval_AverageEpLen : 222.0\n",
      "Train_AverageReturn : 402.22314453125\n",
      "Train_StdReturn : 140.0837860107422\n",
      "Train_MaxReturn : 1079.7421875\n",
      "Train_MinReturn : 212.86961364746094\n",
      "Train_AverageEpLen : 146.3913043478261\n",
      "Train_EnvstepsSoFar : 91570\n",
      "TimeSinceStart : 69.30048990249634\n",
      "Training Loss : -2.0739448070526123\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Running Dagger experiment with seed 1\n",
      "########################\n",
      "logging outputs to  logs/dagger/Hopper/seed1\n",
      "########################\n",
      "Using CPU for this assignment. There may be some bugs with using GPU that cause test cases to not match. You can uncomment the code below if you want to try using it.\n",
      "Hopper-v2\n",
      "Loading expert policy from... deeprl/policies/experts/Hopper.pkl\n",
      "obs (1, 11) (1, 11)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 1029.3245849609375\n",
      "Eval_StdReturn : 106.21780395507812\n",
      "Eval_MaxReturn : 1165.327880859375\n",
      "Eval_MinReturn : 906.8502807617188\n",
      "Eval_AverageEpLen : 299.25\n",
      "Train_AverageReturn : 3772.67041015625\n",
      "Train_StdReturn : 1.9483642578125\n",
      "Train_MaxReturn : 3774.61865234375\n",
      "Train_MinReturn : 3770.721923828125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 1.8373665809631348\n",
      "Training Loss : -3.559098243713379\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     10294 / 10000\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 771.7017822265625\n",
      "Eval_StdReturn : 400.3778991699219\n",
      "Eval_MaxReturn : 1346.8878173828125\n",
      "Eval_MinReturn : 438.3394775390625\n",
      "Eval_AverageEpLen : 255.4\n",
      "Train_AverageReturn : 992.2555541992188\n",
      "Train_StdReturn : 197.84237670898438\n",
      "Train_MaxReturn : 1667.40869140625\n",
      "Train_MinReturn : 391.586181640625\n",
      "Train_AverageEpLen : 294.1142857142857\n",
      "Train_EnvstepsSoFar : 10294\n",
      "TimeSinceStart : 9.064022064208984\n",
      "Training Loss : -2.9829094409942627\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     10257 / 10000\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 535.667236328125\n",
      "Eval_StdReturn : 165.32162475585938\n",
      "Eval_MaxReturn : 714.7271118164062\n",
      "Eval_MinReturn : 235.26089477539062\n",
      "Eval_AverageEpLen : 193.33333333333334\n",
      "Train_AverageReturn : 745.0498657226562\n",
      "Train_StdReturn : 285.6336669921875\n",
      "Train_MaxReturn : 1448.5162353515625\n",
      "Train_MinReturn : 382.74322509765625\n",
      "Train_AverageEpLen : 244.21428571428572\n",
      "Train_EnvstepsSoFar : 20551\n",
      "TimeSinceStart : 16.371243476867676\n",
      "Training Loss : -2.5281198024749756\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     10169 / 10000\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 561.3084716796875\n",
      "Eval_StdReturn : 121.8429946899414\n",
      "Eval_MaxReturn : 738.60791015625\n",
      "Eval_MinReturn : 435.03314208984375\n",
      "Eval_AverageEpLen : 207.0\n",
      "Train_AverageReturn : 495.49908447265625\n",
      "Train_StdReturn : 227.13475036621094\n",
      "Train_MaxReturn : 1198.0115966796875\n",
      "Train_MinReturn : 200.28994750976562\n",
      "Train_AverageEpLen : 184.8909090909091\n",
      "Train_EnvstepsSoFar : 30720\n",
      "TimeSinceStart : 23.601794958114624\n",
      "Training Loss : -2.921133518218994\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     10207 / 10000\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 723.8095703125\n",
      "Eval_StdReturn : 618.68701171875\n",
      "Eval_MaxReturn : 1938.1693115234375\n",
      "Eval_MinReturn : 236.87709045410156\n",
      "Eval_AverageEpLen : 239.2\n",
      "Train_AverageReturn : 559.7381591796875\n",
      "Train_StdReturn : 178.2607421875\n",
      "Train_MaxReturn : 959.1126708984375\n",
      "Train_MinReturn : 227.88807678222656\n",
      "Train_AverageEpLen : 200.13725490196077\n",
      "Train_EnvstepsSoFar : 40927\n",
      "TimeSinceStart : 31.490511178970337\n",
      "Training Loss : -2.4811577796936035\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     10097 / 10000\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 876.7076416015625\n",
      "Eval_StdReturn : 391.15350341796875\n",
      "Eval_MaxReturn : 1483.445556640625\n",
      "Eval_MinReturn : 419.83538818359375\n",
      "Eval_AverageEpLen : 283.75\n",
      "Train_AverageReturn : 469.9319152832031\n",
      "Train_StdReturn : 206.2707977294922\n",
      "Train_MaxReturn : 1156.6060791015625\n",
      "Train_MinReturn : 220.1978759765625\n",
      "Train_AverageEpLen : 180.30357142857142\n",
      "Train_EnvstepsSoFar : 51024\n",
      "TimeSinceStart : 39.343974351882935\n",
      "Training Loss : -2.4721734523773193\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     10067 / 10000\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 538.0322875976562\n",
      "Eval_StdReturn : 247.13768005371094\n",
      "Eval_MaxReturn : 939.8211059570312\n",
      "Eval_MinReturn : 195.01902770996094\n",
      "Eval_AverageEpLen : 200.33333333333334\n",
      "Train_AverageReturn : 722.2638549804688\n",
      "Train_StdReturn : 310.2046203613281\n",
      "Train_MaxReturn : 1791.613525390625\n",
      "Train_MinReturn : 230.7031707763672\n",
      "Train_AverageEpLen : 239.6904761904762\n",
      "Train_EnvstepsSoFar : 61091\n",
      "TimeSinceStart : 47.89522385597229\n",
      "Training Loss : -2.2494912147521973\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     10091 / 10000\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 627.3981323242188\n",
      "Eval_StdReturn : 209.79042053222656\n",
      "Eval_MaxReturn : 955.448486328125\n",
      "Eval_MinReturn : 404.69671630859375\n",
      "Eval_AverageEpLen : 217.6\n",
      "Train_AverageReturn : 527.8870849609375\n",
      "Train_StdReturn : 326.4276428222656\n",
      "Train_MaxReturn : 1710.0458984375\n",
      "Train_MinReturn : 200.7433624267578\n",
      "Train_AverageEpLen : 194.05769230769232\n",
      "Train_EnvstepsSoFar : 71182\n",
      "TimeSinceStart : 56.53611636161804\n",
      "Training Loss : -2.129272937774658\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     10052 / 10000\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 480.054443359375\n",
      "Eval_StdReturn : 186.32882690429688\n",
      "Eval_MaxReturn : 802.2714233398438\n",
      "Eval_MinReturn : 196.6646270751953\n",
      "Eval_AverageEpLen : 183.66666666666666\n",
      "Train_AverageReturn : 500.2962951660156\n",
      "Train_StdReturn : 199.68734741210938\n",
      "Train_MaxReturn : 1196.004638671875\n",
      "Train_MinReturn : 204.38836669921875\n",
      "Train_AverageEpLen : 186.14814814814815\n",
      "Train_EnvstepsSoFar : 81234\n",
      "TimeSinceStart : 64.39886498451233\n",
      "Training Loss : -2.2020795345306396\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     10009 / 10000\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 849.66064453125\n",
      "Eval_StdReturn : 375.96331787109375\n",
      "Eval_MaxReturn : 1357.801025390625\n",
      "Eval_MinReturn : 409.00665283203125\n",
      "Eval_AverageEpLen : 300.5\n",
      "Train_AverageReturn : 397.4583740234375\n",
      "Train_StdReturn : 137.88868713378906\n",
      "Train_MaxReturn : 743.9225463867188\n",
      "Train_MinReturn : 199.482421875\n",
      "Train_AverageEpLen : 158.87301587301587\n",
      "Train_EnvstepsSoFar : 91243\n",
      "TimeSinceStart : 72.68639016151428\n",
      "Training Loss : -2.0084152221679688\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "Running Dagger experiment with seed 2\n",
      "########################\n",
      "logging outputs to  logs/dagger/Hopper/seed2\n",
      "########################\n",
      "Using CPU for this assignment. There may be some bugs with using GPU that cause test cases to not match. You can uncomment the code below if you want to try using it.\n",
      "Hopper-v2\n",
      "Loading expert policy from... deeprl/policies/experts/Hopper.pkl\n",
      "obs (1, 11) (1, 11)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 924.69482421875\n",
      "Eval_StdReturn : 197.03053283691406\n",
      "Eval_MaxReturn : 1235.950439453125\n",
      "Eval_MinReturn : 754.0817260742188\n",
      "Eval_AverageEpLen : 293.0\n",
      "Train_AverageReturn : 3772.67041015625\n",
      "Train_StdReturn : 1.9483642578125\n",
      "Train_MaxReturn : 3774.61865234375\n",
      "Train_MinReturn : 3770.721923828125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 1.863804817199707\n",
      "Training Loss : -3.5931427478790283\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     10275 / 10000\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 960.2435302734375\n",
      "Eval_StdReturn : 152.09617614746094\n",
      "Eval_MaxReturn : 1222.969970703125\n",
      "Eval_MinReturn : 854.7454223632812\n",
      "Eval_AverageEpLen : 279.0\n",
      "Train_AverageReturn : 1047.279296875\n",
      "Train_StdReturn : 240.5595245361328\n",
      "Train_MaxReturn : 1529.275146484375\n",
      "Train_MinReturn : 432.699951171875\n",
      "Train_AverageEpLen : 321.09375\n",
      "Train_EnvstepsSoFar : 10275\n",
      "TimeSinceStart : 9.074397563934326\n",
      "Training Loss : -3.3040194511413574\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     10188 / 10000\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 784.204833984375\n",
      "Eval_StdReturn : 224.44134521484375\n",
      "Eval_MaxReturn : 1134.3258056640625\n",
      "Eval_MinReturn : 508.2607116699219\n",
      "Eval_AverageEpLen : 250.25\n",
      "Train_AverageReturn : 932.51171875\n",
      "Train_StdReturn : 180.27304077148438\n",
      "Train_MaxReturn : 1542.405029296875\n",
      "Train_MinReturn : 473.6119079589844\n",
      "Train_AverageEpLen : 275.35135135135135\n",
      "Train_EnvstepsSoFar : 20463\n",
      "TimeSinceStart : 16.319694757461548\n",
      "Training Loss : -2.8420286178588867\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     10047 / 10000\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 1220.1483154296875\n",
      "Eval_StdReturn : 324.9420166015625\n",
      "Eval_MaxReturn : 1672.95703125\n",
      "Eval_MinReturn : 925.8896484375\n",
      "Eval_AverageEpLen : 362.6666666666667\n",
      "Train_AverageReturn : 927.4203491210938\n",
      "Train_StdReturn : 214.7993927001953\n",
      "Train_MaxReturn : 1332.8519287109375\n",
      "Train_MinReturn : 457.37005615234375\n",
      "Train_AverageEpLen : 279.0833333333333\n",
      "Train_EnvstepsSoFar : 30510\n",
      "TimeSinceStart : 23.51200222969055\n",
      "Training Loss : -2.861232280731201\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     10169 / 10000\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 944.086669921875\n",
      "Eval_StdReturn : 100.95723724365234\n",
      "Eval_MaxReturn : 1117.8056640625\n",
      "Eval_MinReturn : 867.611572265625\n",
      "Eval_AverageEpLen : 277.0\n",
      "Train_AverageReturn : 1019.7166748046875\n",
      "Train_StdReturn : 160.35397338867188\n",
      "Train_MaxReturn : 1422.523681640625\n",
      "Train_MinReturn : 678.2911376953125\n",
      "Train_AverageEpLen : 299.0882352941176\n",
      "Train_EnvstepsSoFar : 40679\n",
      "TimeSinceStart : 30.59622311592102\n",
      "Training Loss : -2.3678150177001953\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     10173 / 10000\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 804.5060424804688\n",
      "Eval_StdReturn : 148.59884643554688\n",
      "Eval_MaxReturn : 925.24951171875\n",
      "Eval_MinReturn : 515.6182861328125\n",
      "Eval_AverageEpLen : 241.2\n",
      "Train_AverageReturn : 929.631103515625\n",
      "Train_StdReturn : 140.0954132080078\n",
      "Train_MaxReturn : 1181.983642578125\n",
      "Train_MinReturn : 520.3265380859375\n",
      "Train_AverageEpLen : 274.94594594594594\n",
      "Train_EnvstepsSoFar : 50852\n",
      "TimeSinceStart : 38.726972579956055\n",
      "Training Loss : -2.352606773376465\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     10242 / 10000\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 699.2435302734375\n",
      "Eval_StdReturn : 212.21585083007812\n",
      "Eval_MaxReturn : 1004.5963134765625\n",
      "Eval_MinReturn : 448.4688415527344\n",
      "Eval_AverageEpLen : 232.2\n",
      "Train_AverageReturn : 848.0548706054688\n",
      "Train_StdReturn : 210.2974395751953\n",
      "Train_MaxReturn : 1171.4720458984375\n",
      "Train_MinReturn : 451.2961730957031\n",
      "Train_AverageEpLen : 256.05\n",
      "Train_EnvstepsSoFar : 61094\n",
      "TimeSinceStart : 46.52801966667175\n",
      "Training Loss : -2.330146551132202\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     10194 / 10000\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 874.05615234375\n",
      "Eval_StdReturn : 226.38510131835938\n",
      "Eval_MaxReturn : 1167.2220458984375\n",
      "Eval_MinReturn : 531.03515625\n",
      "Eval_AverageEpLen : 268.75\n",
      "Train_AverageReturn : 885.7080688476562\n",
      "Train_StdReturn : 213.48757934570312\n",
      "Train_MaxReturn : 1331.73876953125\n",
      "Train_MinReturn : 442.02850341796875\n",
      "Train_AverageEpLen : 268.2631578947368\n",
      "Train_EnvstepsSoFar : 71288\n",
      "TimeSinceStart : 54.57327461242676\n",
      "Training Loss : -2.1420395374298096\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     10094 / 10000\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 779.7658081054688\n",
      "Eval_StdReturn : 145.00437927246094\n",
      "Eval_MaxReturn : 897.7445068359375\n",
      "Eval_MinReturn : 510.53485107421875\n",
      "Eval_AverageEpLen : 238.6\n",
      "Train_AverageReturn : 876.0134887695312\n",
      "Train_StdReturn : 193.75343322753906\n",
      "Train_MaxReturn : 1440.4769287109375\n",
      "Train_MinReturn : 488.2955627441406\n",
      "Train_AverageEpLen : 265.63157894736844\n",
      "Train_EnvstepsSoFar : 81382\n",
      "TimeSinceStart : 63.0572075843811\n",
      "Training Loss : -1.939963698387146\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     10093 / 10000\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 954.986572265625\n",
      "Eval_StdReturn : 156.33966064453125\n",
      "Eval_MaxReturn : 1200.3177490234375\n",
      "Eval_MinReturn : 795.805419921875\n",
      "Eval_AverageEpLen : 281.25\n",
      "Train_AverageReturn : 705.5347290039062\n",
      "Train_StdReturn : 236.92532348632812\n",
      "Train_MaxReturn : 1427.8157958984375\n",
      "Train_MinReturn : 201.71917724609375\n",
      "Train_AverageEpLen : 224.2888888888889\n",
      "Train_EnvstepsSoFar : 91475\n",
      "TimeSinceStart : 71.06579566001892\n",
      "Training Loss : -2.0453219413757324\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Delete all previous logs\n",
    "remove_folder('logs/dagger/{}'.format(env_str))\n",
    "\n",
    "for seed in range(3):\n",
    "    print(\"Running Dagger experiment with seed\", seed)\n",
    "    dagger_args['seed'] = seed\n",
    "    dagger_args['logdir'] = 'logs/dagger/{}/seed{}'.format(env_str, seed)\n",
    "    bctrainer = BC_Trainer(dagger_args)\n",
    "    bctrainer.run_training_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the Dagger results on Hopper, we see that Dagger is able to recover the performance of the expert policy after a few iterations of online interaction and expert relabeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6008 (pid 72790), started 0:00:05 ago. (Use '!kill 72790' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-d49ef92d8baaee50\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-d49ef92d8baaee50\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6008;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Visualize Dagger results on Hopper\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/dagger/Hopper"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
